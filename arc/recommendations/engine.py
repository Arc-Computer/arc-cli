"""
Pure LLM-Based Recommendation Engine

This engine uses only the generic analyzer with LLM to generate recommendations
for any agent configuration based on actual simulation data.
"""

import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import asyncio

from .generic_analyzer import GenericAgentAnalyzer, SimulationContext
from .data_extractor import SimulationDataExtractor
from .llm_client import create_llm_client

logger = logging.getLogger(__name__)


@dataclass
class RecommendationRequest:
    """Request for generating recommendations."""
    failures: List[Dict[str, Any]]
    trajectories: List[Dict[str, Any]]
    scenarios: List[Dict[str, Any]]
    config_path: str
    run_id: Optional[str] = None


@dataclass
class RecommendationResponse:
    """Response containing LLM-generated recommendations."""
    recommendations: List[Dict[str, Any]]
    confidence_score: float
    analysis_summary: Dict[str, Any]
    processing_time: float


class RecommendationEngine:
    """Pure LLM-based recommendation engine that works with any agent configuration."""
    
    def __init__(self, llm_client=None, llm_provider: str = "openrouter"):
        """Initialize with LLM client."""
        self.llm_client = llm_client or create_llm_client(llm_provider)
        
        if not self.llm_client:
            raise ValueError("LLM client is required for the recommendation engine")
        
        # Initialize components
        self.generic_analyzer = GenericAgentAnalyzer(self.llm_client)
        self.data_extractor = SimulationDataExtractor()
        
        logger.info("Initialized pure LLM-based recommendation engine")
    
    async def generate_recommendations(self, request: RecommendationRequest) -> RecommendationResponse:
        """Generate recommendations using only LLM-based analysis."""
        start_time = asyncio.get_event_loop().time()
        
        try:
            logger.info(f"Processing recommendation request for run: {request.run_id}")
            
            # Create simulation context from request data
            simulation_context = self._create_simulation_context(request)
            
            # Use generic analyzer to get LLM-based recommendations
            generic_recommendations = await self.generic_analyzer.analyze_agent_failures(simulation_context)
            
            if not generic_recommendations:
                logger.warning("No recommendations generated by LLM analyzer")
                return self._create_empty_response(start_time)
            
            # Convert to response format
            formatted_recommendations = self._format_recommendations(generic_recommendations)
            
            # Calculate confidence and summary
            confidence = self._calculate_confidence(generic_recommendations)
            summary = self._generate_summary(generic_recommendations, simulation_context)
            
            processing_time = asyncio.get_event_loop().time() - start_time
            
            logger.info(f"Generated {len(formatted_recommendations)} LLM-based recommendations in {processing_time:.2f}s")
            
            return RecommendationResponse(
                recommendations=formatted_recommendations,
                confidence_score=confidence,
                analysis_summary=summary,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Error generating LLM recommendations: {e}")
            import traceback
            traceback.print_exc()
            
            return self._create_error_response(str(e), start_time)
    
    async def generate_recommendations_from_context(self, simulation_context: SimulationContext) -> RecommendationResponse:
        """Generate recommendations directly from SimulationContext (preferred method)."""
        start_time = asyncio.get_event_loop().time()
        
        try:
            logger.info(f"Processing recommendation request for run: {simulation_context.performance_metrics.get('run_id', 'unknown')}")
            
            # Use generic analyzer directly with simulation context
            generic_recommendations = await self.generic_analyzer.analyze_agent_failures(simulation_context)
            
            if not generic_recommendations:
                logger.warning("No recommendations generated by LLM analyzer")
                return self._create_empty_response(start_time)
            
            # Convert to response format
            formatted_recommendations = self._format_recommendations(generic_recommendations)
            
            # Calculate confidence and summary
            confidence = self._calculate_confidence(generic_recommendations)
            summary = self._generate_summary(generic_recommendations, simulation_context)
            
            processing_time = asyncio.get_event_loop().time() - start_time
            
            logger.info(f"Generated {len(formatted_recommendations)} LLM-based recommendations in {processing_time:.2f}s")
            
            return RecommendationResponse(
                recommendations=formatted_recommendations,
                confidence_score=confidence,
                analysis_summary=summary,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Error generating LLM recommendations: {e}")
            import traceback
            traceback.print_exc()
            
            return self._create_error_response(str(e), start_time)
    
    def _create_simulation_context(self, request: RecommendationRequest) -> SimulationContext:
        """Create simulation context from recommendation request."""
        
        # Load original YAML configuration
        original_yaml = self.data_extractor._load_original_yaml(request.config_path)
        
        # Convert request data to simulation context format
        scenario_data = []
        execution_trajectories = []
        failure_data = []
        success_data = []
        
        # Process all data from request
        for i, (failure, trajectory, scenario) in enumerate(zip(request.failures, request.trajectories, request.scenarios)):
            # Build scenario data
            scenario_info = {
                "name": scenario.get("task_prompt", f"scenario_{i}")[:50] + "..." if len(scenario.get("task_prompt", "")) > 50 else scenario.get("task_prompt", f"scenario_{i}"),
                "task_prompt": scenario.get("task_prompt", ""),
                "expected_tools": scenario.get("expected_tools", []),
                "inferred_domain": scenario.get("domain", "unknown"),
                "complexity_level": scenario.get("complexity_level", "medium")
            }
            scenario_data.append(scenario_info)
            
            # Build trajectory data
            trajectory_info = {
                "status": "error" if failure.get("error_message") else "success",
                "error": failure.get("error_message", ""),
                "full_trajectory": trajectory.get("full_trajectory", []),
                "execution_time_seconds": trajectory.get("execution_time", 0)
            }
            execution_trajectories.append(trajectory_info)
            
            # Categorize as failure or success
            if failure.get("error_message"):
                failure_info = {
                    "scenario_name": scenario_info["name"],
                    "error_message": failure.get("error_message", ""),
                    "task_prompt": scenario.get("task_prompt", ""),
                    "expected_tools": scenario.get("expected_tools", []),
                    "scenario_type": scenario.get("domain", "unknown"),
                    "complexity": scenario.get("complexity_level", "medium"),
                    "execution_time": trajectory.get("execution_time", 0)
                }
                failure_data.append(failure_info)
            else:
                success_info = {
                    "scenario_name": scenario_info["name"],
                    "task_prompt": scenario.get("task_prompt", ""),
                    "expected_tools": scenario.get("expected_tools", []),
                    "scenario_type": scenario.get("domain", "unknown"),
                    "complexity": scenario.get("complexity_level", "medium"),
                    "execution_time": trajectory.get("execution_time", 0)
                }
                success_data.append(success_info)
        
        # Create performance metrics
        total_scenarios = len(request.failures)
        performance_metrics = {
            "reliability_score": len(success_data) / total_scenarios if total_scenarios > 0 else 1.0,
            "total_scenarios": total_scenarios,
            "success_count": len(success_data),
            "failure_count": len(failure_data),
            "run_id": request.run_id or "unknown",
            "config_path": request.config_path
        }
        
        return SimulationContext(
            original_yaml=original_yaml,
            scenario_data=scenario_data,
            execution_trajectories=execution_trajectories,
            failure_data=failure_data,
            success_data=success_data,
            performance_metrics=performance_metrics
        )
    
    def _format_recommendations(self, generic_recommendations) -> List[Dict[str, Any]]:
        """Format generic recommendations for output."""
        formatted = []
        
        for i, rec in enumerate(generic_recommendations):
            formatted_rec = {
                "rank": i + 1,
                "issue_type": "llm_generated_recommendation",
                "confidence": rec.confidence_score,
                "specific_problem": rec.issue_description,
                "recommended_fix": rec.root_cause_analysis,
                "yaml_changes": rec.recommended_yaml_changes,
                "expected_improvement": self._extract_improvement_percentage(rec.expected_impact),
                "evidence": rec.evidence,
                "priority_score": rec.confidence_score,
                "llm_generated": True
            }
            formatted.append(formatted_rec)
        
        return formatted
    
    def _extract_improvement_percentage(self, expected_impact: str) -> float:
        """Extract numerical improvement estimate from impact description."""
        import re
        
        # Look for percentage patterns
        percentage_match = re.search(r'(\d+(?:\.\d+)?)%', expected_impact)
        if percentage_match:
            return float(percentage_match.group(1))
        
        # Look for numerical patterns
        number_match = re.search(r'(\d+(?:\.\d+)?)', expected_impact)
        if number_match:
            num = float(number_match.group(1))
            # If it's a reasonable percentage, use it
            if 0 <= num <= 100:
                return num
            # Otherwise cap it
            return min(num, 50.0)
        
        # Default based on confidence and impact keywords
        impact_lower = expected_impact.lower()
        if "significant" in impact_lower or "major" in impact_lower:
            return 25.0
        elif "moderate" in impact_lower:
            return 15.0
        elif "minor" in impact_lower or "small" in impact_lower:
            return 8.0
        else:
            return 12.0  # Default reasonable estimate
    
    def _calculate_confidence(self, generic_recommendations) -> float:
        """Calculate overall confidence from LLM recommendations."""
        if not generic_recommendations:
            return 0.0
        
        # Average confidence with slight boost for multiple recommendations
        avg_confidence = sum(rec.confidence_score for rec in generic_recommendations) / len(generic_recommendations)
        
        # Small boost for having multiple recommendations (more comprehensive analysis)
        multi_rec_bonus = min(len(generic_recommendations) * 0.05, 0.15)
        
        return min(avg_confidence + multi_rec_bonus, 1.0)
    
    def _generate_summary(self, generic_recommendations, simulation_context) -> Dict[str, Any]:
        """Generate analysis summary."""
        return {
            "analysis_approach": "pure_llm_based",
            "total_recommendations": len(generic_recommendations),
            "high_confidence_recommendations": len([r for r in generic_recommendations if r.confidence_score > 0.8]),
            "medium_confidence_recommendations": len([r for r in generic_recommendations if 0.6 <= r.confidence_score <= 0.8]),
            "simulation_data": {
                "total_scenarios": simulation_context.performance_metrics.get("total_scenarios", 0),
                "failure_count": simulation_context.performance_metrics.get("failure_count", 0),
                "success_count": simulation_context.performance_metrics.get("success_count", 0),
                "reliability_score": simulation_context.performance_metrics.get("reliability_score", 0)
            },
            "yaml_config_analyzed": len(simulation_context.original_yaml) > 0,
            "llm_client_used": str(type(self.llm_client).__name__),
            "data_driven_analysis": True
        }
    
    def _create_empty_response(self, start_time: float) -> RecommendationResponse:
        """Create response when no recommendations are generated."""
        processing_time = asyncio.get_event_loop().time() - start_time
        
        return RecommendationResponse(
            recommendations=[],
            confidence_score=0.0,
            analysis_summary={
                "analysis_approach": "pure_llm_based",
                "total_recommendations": 0,
                "error": "No recommendations generated by LLM analyzer"
            },
            processing_time=processing_time
        )
    
    def _create_error_response(self, error_message: str, start_time: float) -> RecommendationResponse:
        """Create error response."""
        processing_time = asyncio.get_event_loop().time() - start_time
        
        return RecommendationResponse(
            recommendations=[],
            confidence_score=0.0,
            analysis_summary={
                "analysis_approach": "pure_llm_based",
                "error": error_message
            },
            processing_time=processing_time
        ) 